#version 440

#define TRINORMAL_OUTPUT
#define SAMPLING_DECISION_OUTPUT
#include "stdheader.glsl"

#include "pixelcache.glsl"
#include "screenreproject.glsl"
#include "lightsource.glsl"

#define MAX_PATHLENGTH 10
#define CONNECTION_EPSILON 0.0


layout (local_size_x = 64, local_size_y = 1, local_size_z = 1) in;
void main()
{
	uint randomSeed = InitRandomSeed(FrameSeed, gl_GlobalInvocationID.x);
	int lightSampleIndex = int(gl_GlobalInvocationID.x / NumRaysPerLightSample);
	vec4 lightSamplePos_Norm0 = texelFetch(InitialLightSampleBuffer, lightSampleIndex * 2);
	vec4 lightIntensity_Norm1 = texelFetch(InitialLightSampleBuffer, lightSampleIndex * 2 + 1);


	// Keep in mind that we are tracing flux = photons!
	// TotalFlux = Integral(I0 * cos Angle) dangle = I0 * PI
	vec3 pathThroughput = lightIntensity_Norm1.xyz * (PI / NumRaysPerLightSample);
	
	Ray ray;
	ray.Origin = lightSamplePos_Norm0.xyz;
	vec3 normal = UnpackNormal(vec2(lightSamplePos_Norm0.w, lightIntensity_Norm1.w));
	vec3 U, V;
	CreateONB(normal, U, V);
	ray.Direction = SampleHemisphereCosine(Random2(randomSeed), U, V, normal);
	
	// Choose a single pixel-cache for the entire light-path
	ivec2 cachePixelCoord = ivec2(RandomUInt(randomSeed) % BackbufferSize.x, RandomUInt(randomSeed) % BackbufferSize.y);
	int cacheIdx = cachePixelCoord.y * BackbufferSize.x + cachePixelCoord.x;
	vec3 barycentricCoordCache = vec3(PixelCacheEntries[cacheIdx].Barycentric0,
									  PixelCacheEntries[cacheIdx].Barycentric1,
									  1 - PixelCacheEntries[cacheIdx].Barycentric0 - PixelCacheEntries[cacheIdx].Barycentric1);
	vec3 cachePosition = texelFetch(VertexPositionBuffer, PixelCacheEntries[cacheIdx].triangle.x).xyz * barycentricCoordCache.x
			+ texelFetch(VertexPositionBuffer, PixelCacheEntries[cacheIdx].triangle.y).xyz * barycentricCoordCache.y
			+ texelFetch(VertexPositionBuffer, PixelCacheEntries[cacheIdx].triangle.z).xyz * barycentricCoordCache.z;
	vec3 cacheNormal; vec2 texcoord;
	GetTriangleHitInfo(PixelCacheEntries[cacheIdx].triangle, barycentricCoordCache, cacheNormal, texcoord);
	MaterialTextureData cacheMaterial = SampleMaterialData(PixelCacheEntries[cacheIdx].triangle.w, texcoord);
	vec3 viewDir = UnpackNormal(PixelCacheEntries[cacheIdx].Incident01);
	
	// Compute direct lighting for the chosen cache
	vec3 cacheColor = EstimateDirectLight(cachePosition, cacheNormal, lightSamplePos_Norm0.xyz, normal,
									lightIntensity_Norm1.xyz, viewDir,
									PixelCacheEntries[cacheIdx].triangle.w, cacheMaterial);
	cacheColor *= LightRayPixelWeight / NumRaysPerLightSample;
	//cacheColor = vec3(0.0);
	//cacheColor *= PixelCacheEntries[cacheIdx].PathThroughput;
	//WriteAtomic(cacheColor, cachePixelCoord);return;
	
	bool lastBounceDiffuse = true;

	for(int i=0; i<MAX_PATHLENGTH; ++i)
	{
		// Trace ray.
		Triangle triangle;
		vec3 barycentricCoord;
		float rayHit = RAY_MAX;
		vec3 geometryNormal;
		TraceRay(ray, rayHit, barycentricCoord, triangle, geometryNormal);
		if(rayHit == RAY_MAX)
			break;
		//geometryNormal = normalize(geometryNormal); geometryNormal is not normalize, but it actually does not need to be! See AdjointBSDFShadingNormalX functions
		ray.Origin = ray.Origin + rayHit * ray.Direction;	// Go to surface with recursive ray (no epsilon yet!)

		// Compute hit normal and texture coordinate.
		vec3 shadingNormal; vec2 hitTexcoord;
		GetTriangleHitInfo(triangle, barycentricCoord, shadingNormal, hitTexcoord);

		// Get Material infos.
		MaterialTextureData materialTexData = SampleMaterialData(triangle.w, hitTexcoord);
		// Diffuse != Diffuse reflectance because of multilayer model. Upper layers might reflect earlier
		float cosInToShading = dot(shadingNormal, ray.Direction);
		float absCosInToShading = abs(dot(shadingNormal, ray.Direction));
		cosInToShading = saturate(-cosInToShading);
		vec3 diffuse = GetDiffuseReflectance(triangle.w, materialTexData, absCosInToShading);
		float avgDiffuse = GetLuminance(diffuse);

		if(avgDiffuse > 0.0 && i>0)// && !lastBounceDiffuse)
		{
			// Try to connect to camera.
			Ray cameraRay;
			cameraRay.Direction = CameraPosition - ray.Origin;
			float camViewDotRay = -dot(CameraW, cameraRay.Direction);
			if(camViewDotRay > 0.0) // Camera faces point?
			{
				// Compute screen coord for this ray.
				vec3 proj = vec3(-dot(CameraU, cameraRay.Direction), -dot(CameraV, cameraRay.Direction), camViewDotRay);
				vec2 screenCoord = proj.xy / proj.z;
				screenCoord.x /= dot(CameraU,CameraU);
				screenCoord.y /= dot(CameraV,CameraV);

				// Valid screen coords?
				if(screenCoord.x >= -1.0 && screenCoord.x < 1.0 && screenCoord.y > -1.0 && screenCoord.y <= 1.0)
				{
					float cameraDistSq = dot(cameraRay.Direction, cameraRay.Direction);
					float cameraDist = sqrt(cameraDistSq);
					cameraRay.Direction /= cameraDist;
					cameraRay.Origin = cameraRay.Direction * RAY_HIT_EPSILON + ray.Origin;

					// Hits *pinhole* camera
					if(!TraceRayAnyHit(cameraRay, cameraDist))
					{
						// Compute pdf conversion factor from image plane area to surface area.
						// The problem: A pixel sees more than only a point! -> photometric distance law (E = I cosTheta / dÂ²) not directly applicable!
						// -> We need the solid angle and the projected area of the pixel in this point!
						// Formulas explained in (6.2) http://www.maw.dk/3d_graphics_projects/downloads/Bidirectional%20Path%20Tracing%202009.pdf
						// Also a look into SmallVCM is a good idea!
						float cosAtCamera = saturate(camViewDotRay / cameraDist); // = dot(-cameraRay.Direction, CameraW);
						cosAtCamera *= cosAtCamera;
						float fluxToIrradiance = AdjointBSDFShadingNormalCorrectedOutDotN(ray.Direction, cameraRay.Direction, geometryNormal, shadingNormal) /
												(cosAtCamera * cosAtCamera * PixelArea * cameraDistSq + DIVISOR_EPSILON);

						//vec3 bsdf = AdjointBSDF(ray.Direction, cameraRay.Direction, triangle.w, materialTexData, shadingNormal);
						//* cosInToShading * saturate(dot(cameraRay.Direction, shadingNormal))
						vec3 color = pathThroughput * (fluxToIrradiance / PI) * diffuse;
						//vec3 color = pathThroughput * (fluxToIrradiance) * bsdf;

						// Add Sample.
						ivec2 pixelCoord = ivec2((screenCoord*0.5 + vec2(0.5)) * BackbufferSize);

						WriteAtomic(color, pixelCoord);
					}
				}
			}
		}//*/
		
		// "Next event estimation" connect to a random pixel via cache
		// Connect the two path vertices and make a visibility test
		if(!PixelCacheEntries[cacheIdx].DirectlyVisible && PixelCacheEntries[cacheIdx].PathThroughput != vec3(0.0) && avgDiffuse > 0.0)// && lastBounceDiffuse)
		{
			vec3 intensity = diffuse * pathThroughput * (cosInToShading / PI);
			vec3 color = EstimateDirectLight(cachePosition, cacheNormal, ray.Origin, shadingNormal,
									intensity, viewDir,
									PixelCacheEntries[cacheIdx].triangle.w, cacheMaterial);
			color *= LightRayPixelWeight * 2;
			cacheColor += color;
		}//*/

		// Continue ray.
		vec3 throughput = vec3(1.0);
		vec3 newDirection = SampleAdjointBSDF(ray.Direction, triangle.w, materialTexData, randomSeed, shadingNormal, throughput, lastBounceDiffuse);
		throughput *= AdjointBSDFShadingNormalCorrection(ray.Direction, newDirection, geometryNormal, shadingNormal);
		//	* abs(dot(ray.Direction, shadingNormal) * dot(newDirection, shadingNormal));
		//pathThroughput *= absCosInToShading;
		//pathThroughput *= abs(dot(newDirection, shadingNormal));
		pathThroughput *= throughput;

	#ifdef RUSSIAN_ROULETTE
		float continuationPropability = saturate(GetLuminance(throughput));
		if(Random(randomSeed) >= continuationPropability) // if terminationPropability is zero, path should be stoped -> >=
			break;
		pathThroughput /= continuationPropability + DIVISOR_EPSILON; // Only change in spectrum, no energy loss. Photon (!) stays the same.
	#endif

		ray.Direction = newDirection;
		ray.Origin += ray.Direction * RAY_HIT_EPSILON;
	}
	
	cacheColor *= PixelCacheEntries[cacheIdx].PathThroughput;
	WriteAtomic(cacheColor, cachePixelCoord);
}